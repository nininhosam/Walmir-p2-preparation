{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11365c18",
   "metadata": {},
   "source": [
    "# DEPENDÊNCIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0e89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install spacy\n",
    "#! pip install nltk\n",
    "#! pip install pandas\n",
    "#! pip install pdfplumber\n",
    "#! pip install chromadb\n",
    "#! pip install sentence-transformers\n",
    "#! pip install openpyxl\n",
    "#! pip install python-docx\n",
    "#! pip install langchain-text-splitters\n",
    "#! python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f60ae3",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "be702a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3bc94f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"aprendizado de maquina.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee57ff90",
   "metadata": {},
   "source": [
    "# Carrega PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5d898489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linhas: 26\n",
      "Caractéres: 1903\n",
      "Prévia do PDF (Primeiros 500 caractéres):\n",
      "\n",
      "APRENDIZAGEM DE MÁQUINA\n",
      "O aprendizado de máquina (AM) é um segmento da Inteligência Artificial que possui\n",
      "um elemento essencial para um comportamento inteligente, a capacidade de\n",
      "aprendizado. A área de AM é responsável por pesquisar métodos computacionais\n",
      "adequados para a aquisição de novos conhecimentos, novas habilidades e novas\n",
      "formas de organização do conhecimento já existente. O aprendizado possibilita que\n",
      "o sistema faça a mesma tarefa ou tarefas sobre uma mesma população de uma\n",
      "maneira mai\n"
     ]
    }
   ],
   "source": [
    "# utiliza a library pdfplumber para extrair o texto do PDF, \n",
    "# o resultado é guardado na variável raw_text\n",
    "def load_pdf(filepath):\n",
    "    with pdfplumber.open(filepath) as pdf:\n",
    "        lines = 0;\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "            lines += text.count(\"\\n\")\n",
    "\n",
    "        print(\"Linhas:\", lines)\n",
    "        print(\"Caractéres:\", len(pdf.chars))\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "raw_text = load_pdf(pdf_path)\n",
    "print(\"Prévia do PDF (Primeiros 500 caractéres):\\n\\n\" + raw_text[:500])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c26f6",
   "metadata": {},
   "source": [
    "# PLN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eaf434",
   "metadata": {},
   "source": [
    "## Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "87e2a63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto em lowercase (prévia):\n",
      "\n",
      " aprendizagem de máquina\n",
      "o aprendizado de máquina (am) é um segmento da inteligência artificial que possui\n",
      "um elemento essencial para um comportamento inteligente, a capacidade de\n",
      "aprendizado. a área de am é responsável por pesquisar métodos computacionais\n",
      "adequados para a aquisição de novos conhecimentos, novas habilidades e novas\n",
      "formas de organização do conhecimento já existente. o aprendizado possibilita que\n",
      "o sistema faça a mesma tarefa ou tarefas sobre uma mesma população de uma\n",
      "maneira mais eficiente a cada execução.\n",
      "o campo do aprendizado de máquina é concebido pela questão de como construir\n",
      "programas, que automaticamente melhoram com a sua experiência [michell,\n",
      "1997]. segundo batista: “aprendizado de máquina – am – é uma subárea de\n",
      "pesquisa muito importante em inteligência artifici\n"
     ]
    }
   ],
   "source": [
    "# transforma todo o texto em letras minúsculas\n",
    "# isso é feito para normalizar o texto, e criar menos variação.\n",
    "text_lower = raw_text.lower()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Texto em lowercase (prévia):\\n\\n\", text_lower[:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7197fd",
   "metadata": {},
   "source": [
    "## Remover caracteres especiais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "83cc7fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto sem caracteres especiais:\n",
      "\n",
      " aprendizagem de maquina o aprendizado de maquina am e um segmento da inteligencia artificial que possui um elemento essencial para um comportamento inteligente, a capacidade de aprendizado. a area de am e responsavel por pesquisar metodos computacionais adequados para a aquisicao de novos conhecimentos, novas habilidades e novas formas de organizacao do conhecimento ja existente. o aprendizado possibilita que o sistema faca a mesma tarefa ou tarefas sobre uma mesma populacao de uma maneira mais eficiente a cada execucao. o campo do aprendizado de maquina e concebido pela questao de como construir programas, que automaticamente melhoram com a sua experiencia michell, 1997 . segundo batista aprendizado de maquina am e uma subarea de pesquisa muito importante em inteligencia artificial ia poi\n"
     ]
    }
   ],
   "source": [
    "def remove_special_chars(text):\n",
    "    # normaliza acentos (NFKD)\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = text.encode(\"ASCII\", \"ignore\").decode(\"utf-8\")\n",
    "\n",
    "    # remove caracteres que não sejam letras, numeros ou espaços\n",
    "    # deixo também ponto-final e virgula, para usar o \n",
    "    # método de chunk recursivo posteriormente\n",
    "    text = re.sub(r\"[^a-z0-9.,\\s]\", \" \", text)\n",
    "\n",
    "    # remove múltiplos espaços\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text_clean = remove_special_chars(text_lower)\n",
    "\n",
    "print(\"Texto sem caracteres especiais:\\n\\n\", text_clean[:800])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b7c3b",
   "metadata": {},
   "source": [
    "## Remover Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "96569c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens (todos) (primeiros 30):        [aprendizagem, de, maquina, o, aprendizado, de, maquina, am, e, um, segmento, da, inteligencia, artificial, que, possui, um, elemento, essencial, para, um, comportamento, inteligente, ,, a, capacidade, de, aprendizado, ., a]\n",
      "Tokens sem stopwords (primeiros 30):  [aprendizagem, maquina, aprendizado, maquina, am, segmento, inteligencia, artificial, possui, elemento, essencial, comportamento, inteligente, ,, capacidade, aprendizado, ., area, am, responsavel, pesquisar, metodos, computacionais, adequados, aquisicao, novos, conhecimentos, ,, novas, habilidades]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\notxy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\notxy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\notxy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# tokenização com spaCy pois funciona melhor com português, em comparação com nltk.\n",
    "# stopwords foram resgatadas com o NLTK, \n",
    "# pois a lista de stopwords em português do spaCy \n",
    "# não foi tão boa quanto a do NLTK nos testes que fiz\n",
    "\n",
    "\n",
    "\n",
    "# carrega o modelo de português\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "# tokeniza utilizando spacy\n",
    "token = nlp(text_clean)\n",
    "tokens_with_stop = [t for t in token]\n",
    "\n",
    "# baixa os recursos necessários do NLTK para stopwords, e as remove.\n",
    "# PS: stopwords são palavras que não agregam para o contexto, e podem ser filtradas.\n",
    "# palavras como [\"a\", \"o\", \"de\", \"da\", \"na\", etc...]\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "stopwords_pt = set(stopwords.words(\"portuguese\"))\n",
    "\n",
    "# é possível adicionar stopwords ao conjunto, por exemplo, nesse texto existe uma referência: (2003, p. 11)\n",
    "# caso queira filtrar o \"p.\", que não agrega muito valor, basta fazer isto:\n",
    "stopwords_pt.add(\"p.\")\n",
    "\n",
    "tokens_no_stop = [t for t in token if t.text not in stopwords_pt]\n",
    "\n",
    "print(\"\\nTokens (todos) (primeiros 30):       \", tokens_with_stop[:30])\n",
    "print(\"Tokens sem stopwords (primeiros 30): \", tokens_no_stop[:30])\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77506d",
   "metadata": {},
   "source": [
    "## Lemmatização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5c90f043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmas (primeiros 30):\n",
      "\n",
      " aprendizagem maquina aprendizado maquina am segmento inteligencia artificial possuir elemento essencial comportamento inteligente , capacidade aprendizado . areo Am responsavel pesquisar metodo computacional adequar aquisicao novo conhecimento , novo habilidade novo forma organizacao conhecimento ja\n"
     ]
    }
   ],
   "source": [
    "# lemmatização utilizando spaCy, pegando o lemma_ dos tokens da ultima etapa.\n",
    "# utilizando lemmatização, pois ela os preserva melhor \n",
    "# a semântica em comparação com stemming. \n",
    "# stemming não funciona tão bem quando é necessário manter a semântica das palavras.\n",
    "\n",
    "def lemmatize_text(tokens):\n",
    "    return [token.lemma_ for token in tokens]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lemmas = \" \".join(lemmatize_text(tokens_no_stop))\n",
    "\n",
    "print(\"Lemmas (primeiros 30):\\n\\n\", lemmas[:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f06223c",
   "metadata": {},
   "source": [
    "# Chunking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eece82",
   "metadata": {},
   "source": [
    "### Recursívo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f5b1e1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks: 24\n",
      "\n",
      " ['aprendizagem maquina aprendizado maquina am segmento inteligencia artificial possuir elemento', 'elemento essencial comportamento inteligente', ', capacidade aprendizado', '. areo Am responsavel pesquisar metodo computacional adequar aquisicao novo conhecimento', ', novo habilidade novo forma organizacao conhecimento ja existente', '. aprendizado possibilito sistema facar mesmo tarefa tarefa sobre mesmo populacao maneira eficiente', 'eficiente cada execucao', '. campo aprendizado maquina conceber questao construir programa', ', automaticamente melhor experiencia michell , 1997', '. segundo batista aprendizar maquina am subarea pesquisa importante inteligencia artificial ir pois', 'ir pois capacidade aprender essencial comportamento inteligente', '. Am tratar estudo metor computacional adquirir novo conhecimento', ', novo habilidade novo meio organizar conhecimento ja existente', '. 2003 , 11', '. aprendizado supervisionar ocorrer conjunto exemplo sao fornecer sistema respectivo classe', ', objetivo classificar novo conjunto ainda nao rotular', '. problema de este modo encontrar conjunto exemplo satisfatorio permitir condicao', '. aprendizar supervisionar atividade predicao classificacao previsao classe discreta pre definido', 'definido regressao previsao valor numerico continuer algoritmo aprendizar indutor receber conjunto', 'conjunto exemplo treinamento qual rotulo classe associar sao conhecer', '. cada exemplo instancia padrao descrever vetor valor atributo rotulo classe associar', '. objetivo indutor construir classificador poder determinar corretamente classe novo exemplo ainda', 'ainda nao rotular', '.']\n"
     ]
    }
   ],
   "source": [
    "# técnica utilizada: chunking recursivo\n",
    "# divide o texto em chunks utilizando RecursiveCharacterTextSplitter do LangChain\n",
    "\n",
    "# corta o texto, tentando preservar a integridade semântica dos pedaços, \n",
    "# utilizando uma lista de separadores\n",
    "\n",
    "# os separadores usados nesse caso foram: \n",
    "# dupla quebra de linha (para titulos e paragrafos), \n",
    "# quebra de linha padrão, \n",
    "# ponto e virgula para frases, \n",
    "# e espaços para palavras.\n",
    "\n",
    "# isso visa ir do mais amplo para o mais específico.\n",
    "# o chunk size de 100 foi escolhido pelo tamanho do texto, que é relativamente pequeno\n",
    "# o overlap de 10 é 10% do chunk-size.           (10-20% do chunk-size é recomendado.)\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \",\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chunks_recursive = recursive_splitter.split_text(lemmas)\n",
    "\n",
    "print(f\"Total de chunks: {len(chunks_recursive)}\\n\\n\", chunks_recursive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e595fb5",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac860a3",
   "metadata": {},
   "source": [
    "## Hugging Face SentenceTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "be04e13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensão: (24, 384)\n",
      "\n",
      "Resultado: \n",
      " [[-0.05333175 -0.17737345 -0.0476945  ...  0.15515457 -0.0615542\n",
      "  -0.173321  ]\n",
      " [-0.00532783  0.15110852 -0.2187479  ...  0.46550682 -0.05833727\n",
      "   0.04138125]\n",
      " [ 0.25355896 -0.11605787 -0.29423347 ...  0.17351723 -0.1914304\n",
      "  -0.00337322]\n",
      " [-0.20642872 -0.02353211 -0.172113   ...  0.10338047 -0.11056297\n",
      "  -0.100933  ]\n",
      " [-0.10843997 -0.18992469 -0.2598444  ...  0.05734963 -0.02787761\n",
      "   0.18699992]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# transforma os chunks em embeddings, para que possam ser inseridos no banco vetorial.\n",
    "# utiliza o modelo pré-treinado do sentence transformers para gerar embeddings\n",
    "\n",
    "# escolhi o modelo \"paraphrase-multilingual-MiniLM-L12-v2\" por ser leve \n",
    "# e ter boa performance em múltiplos idiomas.\n",
    "# nos testes que fiz, foi o que se saiu melhor.\n",
    "\n",
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "def embed_hf(chunks):\n",
    "    vectors = model.encode(chunks, show_progress_bar=True)\n",
    "\n",
    "    print(f\"\\nDimensão: {vectors.shape}\\n\\nResultado: \\n\", vectors[:5])\n",
    "\n",
    "    return vectors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embedding = embed_hf(chunks_recursive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2468f44e",
   "metadata": {},
   "source": [
    "# Banco Vetorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b2d427",
   "metadata": {},
   "source": [
    "## Criação/Inserção ChromaDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "908827f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 documentos na coleção\n"
     ]
    }
   ],
   "source": [
    "# inicializa o cliente ChromaDB e cria uma coleção para armazenar os embeddings\n",
    "client = chromadb.Client()\n",
    "collection_name = \"exemplo_colecao\"\n",
    "\n",
    "# resetando coleção caso exista (para testes)\n",
    "if collection_name in [col.name for col in client.list_collections()]:\n",
    "    client.delete_collection(collection_name)\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=None,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "# metadata especifica que vai utilizar o método de cosine, que é o método que\n",
    "# o sentence-transformers e o modelo utilizam.\n",
    "\n",
    "\n",
    "\n",
    "print(f\"{collection.count()} documentos na coleção\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3017c9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings inseridos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Insere os embeddings na coleção do ChromaDB, criando IDs únicos para cada chunk\n",
    "ids = [f\"chunk_{i}\" for i in range(len(chunks_recursive))]\n",
    "\n",
    "collection.add(\n",
    "    ids=ids,\n",
    "    documents=chunks_recursive,\n",
    "    embeddings=embedding\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Embeddings inseridos com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c493f4",
   "metadata": {},
   "source": [
    "## Busca Semântica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ae410792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pergunta: o que é aprendizado de máquina?\n",
      "(Score: 0.2497009038925171) - Documento: aprendizagem maquina aprendizado maquina am segmento inteligencia artificial possuir elemento\n",
      "(Score: 0.27182525396347046) - Documento: . campo aprendizado maquina conceber questao construir programa\n",
      "(Score: 0.43537425994873047) - Documento: , capacidade aprendizado\n",
      "\n",
      "Pergunta: o que é aprendizado supervisionado?\n",
      "(Score: 0.1887296438217163) - Documento: . aprendizado supervisionar ocorrer conjunto exemplo sao fornecer sistema respectivo classe\n",
      "(Score: 0.2815517783164978) - Documento: , capacidade aprendizado\n",
      "(Score: 0.30614417791366577) - Documento: . aprendizar supervisionar atividade predicao classificacao previsao classe discreta pre definido\n"
     ]
    }
   ],
   "source": [
    "# realiza uma consulta de similaridade no banco vetorial utilizando uma pergunta\n",
    "questions = [\"o que é aprendizado de máquina?\", \"o que é aprendizado supervisionado?\"]\n",
    "for question in questions:\n",
    "    # pega uma pergunta, e converte em embedding, no mesmo modelo usado para criar \n",
    "    # os embeddings dos chunks.\n",
    "    question_embedding = model.encode([question])\n",
    "\n",
    "    # realiza a consulta no banco vetorial utilizando o embedding da pergunta, \n",
    "    # e pega os 3 resultados mais similares.\n",
    "    resultados = collection.query(\n",
    "        query_embeddings=question_embedding.tolist(),\n",
    "        n_results=3\n",
    "    )\n",
    "\n",
    "    print(f\"\\nPergunta: {question}\")\n",
    "\n",
    "    # os scores mostram a similaridade entre a pergunta e o chunk retornado. \n",
    "    # Scores mais baixos indicam maior similaridade. (0 é idêntico, 1 é muito diferente)\n",
    "    for doc, score in zip(resultados['documents'][0], resultados['distances'][0]):\n",
    "        print(f\"(Score: {score}) - Documento: {doc}\")\n",
    "\n",
    "\n",
    "# as primeiras respostas (mais precisas) receberam score de 0.249... e 0.188...\n",
    "# elas pegaram corretamente os trechos do texto com que eu esperava, e baseei a pergunta,\n",
    "# então considero que a busca foi um sucesso.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
