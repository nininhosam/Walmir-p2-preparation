{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ebca54",
   "metadata": {},
   "source": [
    "# PRÉ PROCESSAMENTO (PLN)\n",
    "\n",
    "\n",
    "| Técnica              | Como funciona                                                   | Prós                           | Contras                                                            | Quando usar                                             |\n",
    "| -------------------- | --------------------------------------------------------------- | ------------------------------ | ------------------------------------------------------------------ | ------------------------------------------------------- |\n",
    "| Lowercasing          | Converte todo texto para minúsculas                             | Simples; deixa tudo uniforme   | Pode perder informações quando maiúsculas têm significado (siglas) | Quase sempre                                            |\n",
    "| Remover pontuação    | Remove vírgulas, pontos, etc.                                   | Simplifica tokens              | Pode remover significado (ex. \"Olá!?\" → emoção)                    | Quando o significado semântico não depende de pontuação |\n",
    "| Remover stopwords    | Remove palavras comuns que não carregam significado             | Reduz ruído                    | Às vezes piora embeddings modernos                                 | Relevante para modelos clássicos (TF-IDF, Bag-of-words) |\n",
    "| Lematização          | Reduz palavra à forma de dicionário (“caminhando” → “caminhar”) | Melhora consistência semântica | Mais lento; requer modelos                                         | Textos formais, análise semântica                       |\n",
    "| Stemming             | Corta final da palavra (“caminhando” → “caminh”)                | Mais rápido que lemmatizer     | Resultado muitas vezes feio                                        | Quando performance importa                              |\n",
    "| Remoção de HTML      | Remove tags como `<p>`, `<a>`                                   | Necessário em textos web       | Pode remover informação útil                                       | Quando dados vêm da web                                 |\n",
    "| Normalização Unicode | Remove acentos e caracteres especiais                           | Evita erros de encoding        | Perde precisão linguística                                         | Quando sistema não suporta Unicode                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef35f7",
   "metadata": {},
   "source": [
    "# TOKENIZAÇÃO\n",
    "\n",
    "| Método                                        | Como funciona                                          | Prós                        | Contras                                    | Quando usar                    |\n",
    "| --------------------------------------------- | ------------------------------------------------------ | --------------------------- | ------------------------------------------ | ------------------------------ |\n",
    "| Tokenização por espaço                        | Divide por espaço (“texto simples”)                    | Rápido                      | Ignora pontuação, expressões               | Textos simples e curtos        |\n",
    "| Tokenização NLTK                              | Usa regras linguísticas                                | Mais preciso                | Depende de idioma                          | Provas de PLN clássico         |\n",
    "| spaCy tokenizer                               | Tokenização robusta por modelo                         | Muito preciso               | Mais pesado                                | Projetos profissionais         |\n",
    "| Tokenização por sub-palavras (BPE, WordPiece) | Divide palavras em pedaços: “jogando” → “jog” + “ando” | Essencial para transformers | Tokens podem ficar difíceis de interpretar | Usado para embeddings modernos |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e1b962",
   "metadata": {},
   "source": [
    "# CHUNKING\n",
    "\n",
    "| Técnica                                    | Como funciona                                                 | Prós                                | Contras                      | Quando usar                      |\n",
    "| ------------------------------------------ | ------------------------------------------------------------- | ----------------------------------- | ---------------------------- | -------------------------------- |\n",
    "| Chunk por número de caracteres             | Ex.: 500–1.000 chars por bloco                                | Simples; rápido                     | Pode quebrar frases          | Arquivos simples (PDF, TXT)      |\n",
    "| Chunk por número de tokens                 | Ex.: 200–500 tokens                                           | Evita ultrapassar limites do modelo | Requer tokenizador do modelo | Embeddings modernos (OpenAI, HF) |\n",
    "| Chunk por parágrafos                       | Divide em blocos naturais do texto                            | Mantém contexto natural             | Parágrafos podem ser grandes | Textos formais (PDFs acadêmicos) |\n",
    "| Chunk por frases                           | Usa modelo de segmentação (spaCy)                             | Alta coerência                      | Muito granular               | Perguntas de precisão alta       |\n",
    "| RecursiveCharacterTextSplitter (LangChain) | Mistura vários métodos e tenta não cortar contexto importante | Melhor qualidade geral              | Implementação mais complexa  | Pipelines RAG profissionais      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd256d7",
   "metadata": {},
   "source": [
    "# EMBEDDING\n",
    "\n",
    "| Tipo                                      | Como funciona                          | Prós                           | Contras                             | Quando usar                   |\n",
    "| ----------------------------------------- | -------------------------------------- | ------------------------------ | ----------------------------------- | ----------------------------- |\n",
    "| Word Embeddings (Word2Vec, GloVe)         | Cria um vetor por palavra              | Simples                        | Não entende contexto                | Conceitos básicos de NLP      |\n",
    "| Sentence Embeddings (SBERT)               | Vetores para frases completas          | Muito bom para busca semântica | Mais pesado                         | RAG local, sem API            |\n",
    "| Document Embeddings                       | Vetores grandes de documentos inteiros | Capta contexto geral           | Perde precisão em buscas finas      | Textos longos resumidos       |\n",
    "| Embeddings proprietários (OpenAI, Cohere) | Modelos muito treinados                | Alta qualidade; contexto forte | Dependência externa                 | Quando precisão é prioritária |\n",
    "| Embeddings multi-linguagem                | Aceita vários idiomas                  | Ideal para datasets mistos     | Pode ser pior que modelos dedicados | Dados multilíngues            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf790c3",
   "metadata": {},
   "source": [
    "# BANCO VETORIAL\n",
    "\n",
    "| Banco        | Como funciona                      | Prós              | Contras                    | Quando usar                       |\n",
    "| ------------ | ---------------------------------- | ----------------- | -------------------------- | --------------------------------- |\n",
    "| ChromaDB     | Local, simples, Python puro        | Fácil de usar     | Não escalável para milhões | Estudo, protótipos, prova         |\n",
    "| FAISS (Meta) | Biblioteca C++/Python para vetores | Muito rápido      | Sem metadados robustos     | Busca offline de alta performance |\n",
    "| Pinecone     | SaaS de vetores                    | Escalável, rápido | Pago após certo limite     | Produção profissional             |\n",
    "| Qdrant       | Open-source com API                | Muito robusto     | Requer setup               | Projetos intermediários           |\n",
    "| Milvus       | Muito escalável                    | Industrial        | Complexo                   | Big Data, sistemas de busca       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df9e81a",
   "metadata": {},
   "source": [
    "# Busca Semântica\n",
    "\n",
    "| Técnica                            | Como funciona             | Prós                  | Contras                        | Quando usar                     |\n",
    "| ---------------------------------- | ------------------------- | --------------------- | ------------------------------ | ------------------------------- |\n",
    "| Cosine Similarity                  | Mede ângulo entre vetores | Ideal para embeddings | Costoso com milhões de vetores | RAG padrão                      |\n",
    "| Dot Product                        | Produto escalar           | Muito rápido          | Menos intuitivo                | Modelos otimizados para isso    |\n",
    "| L2 Distance (Euclidiana)           | Distância geométrica      | Útil para clusters    | Menos eficaz semanticamente    | Clustering, não busca semântica |\n",
    "| Approximate Nearest Neighbor (ANN) | Busca aproximada          | Ultra-rápido          | Não garante 100% precisão      | Escala de milhões               |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
