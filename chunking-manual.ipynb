{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66bd3156",
   "metadata": {},
   "source": [
    "###  MANUAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29510119",
   "metadata": {},
   "source": [
    "#### Por tamanho fixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38a7c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks: 9\n",
      "Exemplo de chunk:\n",
      "\n",
      "titulo introducao machine Learning machine learning Campo inteligencia artificial desenvolver algori\n"
     ]
    }
   ],
   "source": [
    "# --- CHUNKING POR TAMANHO FIXO (CARACTERES) ---\n",
    "def chunk_by_chars(text, chunk_size=500, overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "chunks_chars = chunk_by_chars(lemmas, chunk_size=100, overlap=10)\n",
    "\n",
    "print(f\"Total de chunks: {len(chunks_chars)}\")\n",
    "print(\"Exemplo de chunk:\\n\")\n",
    "print(chunks_chars[0][:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19c75ba",
   "metadata": {},
   "source": [
    "#### Por tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c999298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks: 1\n",
      "Primeiro par√°grafo:\n",
      "\n",
      "titulo introducao machine Learning machine learning Campo inteligencia artificial desenvolver algoritmos capaz aprender padroe partir dado principal tipo incluir aprendizar supervisionar aprendizar nao supervisionar aprendizar reforco aprendizar supervisionar envolver treinar modelo dado rotular pre\n"
     ]
    }
   ],
   "source": [
    "# --- CHUNKING POR PAR√ÅGRAFOS ---\n",
    "def chunk_by_paragraph(text):\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\") if p.strip()]\n",
    "    return paragraphs\n",
    "\n",
    "chunks_paragraphs = chunk_by_paragraph(lemmas)\n",
    "\n",
    "print(f\"Total de chunks: {len(chunks_paragraphs)}\")\n",
    "print(\"Primeiro par√°grafo:\\n\")\n",
    "print(chunks_paragraphs[0][:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc249918",
   "metadata": {},
   "source": [
    "#### Por Senten√ßas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6313f8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks: 1\n",
      "Exemplo de chunk:\n",
      "\n",
      "titulo introducao machine Learning machine learning Campo inteligencia artificial desenvolver algoritmos capaz aprender padroe partir dado principal tipo incluir aprendizar supervisionar aprendizar nao supervisionar aprendizar reforco aprendizar supervisionar envolver treinar modelo dado rotular pre\n"
     ]
    }
   ],
   "source": [
    "# --- CHUNKING POR SENTEN√áAS ---\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def chunk_by_sentences(text, max_sentences=5):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(sentences), max_sentences):\n",
    "        chunk = \" \".join(sentences[i:i + max_sentences])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks_sentences = chunk_by_sentences(lemmas, max_sentences=5)\n",
    "\n",
    "print(f\"Total de chunks: {len(chunks_sentences)}\")\n",
    "print(\"Exemplo de chunk:\\n\")\n",
    "print(chunks_sentences[0][:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccdb26b",
   "metadata": {},
   "source": [
    "#### H√≠brido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a309d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks: 2\n",
      "Exemplo de chunk:\n",
      "\n",
      "titulo introducao machine Learning machine learning Campo inteligencia artificial desenvolver algoritmos capaz aprender padroe partir dado principal tipo incluir aprendizar supervisionar aprendizar nao supervisionar aprendizar reforco aprendizar supervisionar envolver treinar modelo dado rotular pre\n"
     ]
    }
   ],
   "source": [
    "# --- CHUNKING H√çBRIDO (SENTEN√áAS AT√â LIMITE DE CARACTERES) ---\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def hybrid_sentence_chunk(text, max_chars=500):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "\n",
    "    for sent in sentences:\n",
    "        if len(current) + len(sent) <= max_chars:\n",
    "            current += \" \" + sent\n",
    "        else:\n",
    "            chunks.append(current.strip())\n",
    "            current = sent\n",
    "    \n",
    "    if current:\n",
    "        chunks.append(current.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "chunks_hybrid = hybrid_sentence_chunk(lemmas, max_chars=500)\n",
    "\n",
    "print(f\"Total de chunks: {len(chunks_hybrid)}\")\n",
    "print(\"Exemplo de chunk:\\n\")\n",
    "print(chunks_hybrid[1][:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98455013",
   "metadata": {},
   "source": [
    "#### Recurs√≠vo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27653b7",
   "metadata": {},
   "source": [
    "## Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efed86a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 5\u001b[39m\n",
      "\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m client = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_openai\u001b[39m(chunks, model=\u001b[33m\"\u001b[39m\u001b[33mtext-embedding-3-small\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müî• Embedding usando modelo OpenAI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\github\\Walmir-p2-preparation\\venv\\Lib\\site-packages\\openai\\_client.py:137\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n",
      "\u001b[32m    135\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n",
      "\u001b[32m    138\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m    139\u001b[39m     )\n",
      "\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n",
      "\u001b[32m    141\u001b[39m     \u001b[38;5;28mself\u001b[39m.api_key = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def embed_openai(chunks, model=\"text-embedding-3-small\"):\n",
    "    print(f\"üî• Embedding usando modelo OpenAI: {model}\")\n",
    "    start = time.time()\n",
    "\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=chunks\n",
    "    )\n",
    "\n",
    "    vectors = [item.embedding for item in response.data]\n",
    "    duration = round(time.time() - start, 3)\n",
    "\n",
    "    print(f\"Dimens√£o: {len(vectors[0])}\")\n",
    "    print(f\"Total de chunks: {len(chunks)}\")\n",
    "    print(f\"Tempo: {duration}s\")\n",
    "\n",
    "    return vectors\n",
    "\n",
    "# Teste\n",
    "openai_vectors_small = embed_openai(chunks_recursive, model=\"text-embedding-3-small\")\n",
    "openai_vectors_large = embed_openai(chunks_recursive, model=\"text-embedding-3-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks gerados: 2\n",
      "\n",
      "Exemplo de chunk:\n",
      "\n",
      "titulo introducao machine Learning machine learning Campo inteligencia artificial desenvolver algoritmos capaz aprender padroe partir dado principal tipo incluir aprendizar supervisionar aprendizar nao supervisionar aprendizar reforco aprendizar supervisionar envolver treinar modelo dado rotular prever preco casa base caracteristico tamanho localizacao numero quarto aprendizar nao supervisionar de\n"
     ]
    }
   ],
   "source": [
    "# --- CHUNKING RECURSIVO ---\n",
    "\n",
    "def recursive_chunk(text, max_size=500, overlap=50):\n",
    "    \"\"\"\n",
    "    Implementa√ß√£o simplificada do Recursive Character Text Splitter\n",
    "    estilo LangChain.\n",
    "    \"\"\"\n",
    "\n",
    "    # ordem de separadores: do mais forte ao mais fraco\n",
    "    separators = [\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "\n",
    "    def split_text(text, separators):\n",
    "        \"\"\"Divide texto usando o primeiro separador dispon√≠vel.\"\"\"\n",
    "        for sep in separators:\n",
    "            if sep in text:\n",
    "                return text.split(sep), sep\n",
    "        return [text], None  # nenhum separador encontrado\n",
    "\n",
    "    # ---------- fun√ß√£o recursiva -----------\n",
    "\n",
    "    def _recursive(text):\n",
    "        if len(text) <= max_size:\n",
    "            return [text]\n",
    "\n",
    "        parts, sep = split_text(text, separators)\n",
    "\n",
    "        # Se o texto n√£o tiver nenhum dos separadores\n",
    "        if sep is None:\n",
    "            # chunk por caracteres mesmo\n",
    "            return [\n",
    "                text[i:i+max_size]\n",
    "                for i in range(0, len(text), max_size)\n",
    "            ]\n",
    "\n",
    "        chunks = []\n",
    "        current = \"\"\n",
    "\n",
    "        for part in parts:\n",
    "            # tenta adicionar a parte atual\n",
    "            if len(current) + len(part) + len(sep) <= max_size:\n",
    "                current += part + sep\n",
    "            else:\n",
    "                # se a parte √© maior que o limite,\n",
    "                # precisamos dividir recursivamente\n",
    "                if len(part) > max_size:\n",
    "                    chunks.extend(_recursive(part))\n",
    "                else:\n",
    "                    chunks.append(current.strip())\n",
    "                    current = part + sep\n",
    "\n",
    "        if current.strip():\n",
    "            chunks.append(current.strip())\n",
    "\n",
    "        # aplicar overlap\n",
    "        if overlap > 0:\n",
    "            chunks_with_overlap = []\n",
    "            for i in range(len(chunks)):\n",
    "                chunk = chunks[i]\n",
    "                if i > 0:\n",
    "                    chunk = chunks[i-1][-overlap:] + \" \" + chunk\n",
    "                chunks_with_overlap.append(chunk)\n",
    "            chunks = chunks_with_overlap\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    # ---------------------------------------\n",
    "\n",
    "    return _recursive(text)\n",
    "\n",
    "\n",
    "# ---- executando no seu texto limpo ----\n",
    "chunks_recursive = recursive_chunk(lemmas, max_size=500, overlap=50)\n",
    "\n",
    "print(f\"Total de chunks gerados: {len(chunks_recursive)}\")\n",
    "print(\"\\nExemplo de chunk:\\n\")\n",
    "print(chunks_recursive[0][:400])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
